The multi-armed bandit problem is a classical problem in probability theory and decision theory, which models a situation where a gambler must decide which arm of a multi-armed bandit (slot machine) to pull to maximize their total reward over a sequence of trials. Each arm of the bandit provides a random reward from a probability distribution specific to that arm. The challenge is to balance exploration (trying different arms to learn about their reward distributions) and exploitation (pulling the arm that is currently believed to offer the highest reward) to maximize the total reward over time.

### Key Concepts

1. **Exploration vs. Exploitation:**
   - **Exploration:** Trying out different arms to gather information about their reward distributions.
   - **Exploitation:** Using the current knowledge to pull the arm that is expected to yield the highest reward.

2. **Reward Distribution:** Each arm has an unknown probability distribution of rewards, which the gambler tries to learn over time.

3. **Objective:** The goal is to maximize the cumulative reward over a sequence of pulls.

### Mathematical Formulation

- Let \( k \) be the number of arms.
- Let \( T \) be the total number of pulls.
- Let \( \mu_i \) be the expected reward of arm \( i \).
- The gambler's task is to choose a sequence of pulls that maximizes the expected cumulative reward.

### Algorithms for Solving the Multi-Armed Bandit Problem

1. **ε-Greedy Algorithm:** With probability \( ε \), explore by selecting a random arm. With probability \( 1 - ε \), exploit by selecting the arm with the highest estimated reward.

2. **[[Upper Confidence Bound (UCB)]]:** Selects the arm based on the upper confidence bound of the reward estimates, balancing exploration and exploitation.

3. **Thompson Sampling:** Uses Bayesian inference to update the probability distribution of the rewards and selects arms based on sampling from these distributions.

### Applications

The multi-armed bandit problem has applications in various fields, including:
- Online advertising (choosing which ads to display to maximize click-through rates)
- Clinical trials (allocating patients to different treatments)
- A/B testing (comparing multiple versions of a webpage)
- Adaptive routing in networks

The problem is fundamental in reinforcement learning and provides a framework for making decisions under uncertainty.